# Popular Diffusion models

# DALLE

[unCLIp: Dalle](https://www.notion.so/unCLIp-Dalle-323fede6597443af8d479fb9e5f0f0ef?pvs=21) 

[How DALL-E 2 Actually Works](https://www.assemblyai.com/blog/how-dall-e-2-actually-works/)

# GLIDE

[ä»DDPMåˆ°GLIDEï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆç®—æ³•è¿›å±•](https://zhuanlan.zhihu.com/p/449284962?utm_psn=1730808131335135232)

# Imagen

[Imagen](https://www.notion.so/Imagen-faa04e5bea3d488585f829f89a07e0df?pvs=21) 

# Latent Diffusion Model

[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)

[æ–‡ç”Ÿå›¾æ¨¡å‹ä¹‹Stable Diffusion](https://zhuanlan.zhihu.com/p/617134893?utm_psn=1730824401908662272)

## ç®€ä»‹

è¿™ä¸ªå·¥ä½œï¼Œå¸¸è§„çš„æ‰©æ•£æ¨¡å‹æ˜¯åŸºäºpixelçš„ç”Ÿæˆæ¨¡å‹ï¼Œè€ŒLatent Diffusionæ˜¯åŸºäºlatentçš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒå…ˆé‡‡ç”¨ä¸€ä¸ªautoencoderå°†å›¾åƒå‹ç¼©åˆ°latentç©ºé—´ï¼Œç„¶åç”¨æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå›¾åƒçš„latentsï¼Œæœ€åé€å…¥autoencoderçš„decoderæ¨¡å—å°±å¯ä»¥å¾—åˆ°ç”Ÿæˆçš„å›¾åƒã€‚

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled.png)

SDæ¨¡å‹çš„ä¸»ä½“ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å‹ï¼š

- **autoencoder**ï¼šencoderå°†å›¾åƒå‹ç¼©åˆ°latentç©ºé—´ï¼Œè€Œdecoderå°†latentè§£ç ä¸ºå›¾åƒï¼›
- **CLIP text encoder**ï¼šæå–è¾“å…¥textçš„text embeddingsï¼Œé€šè¿‡cross attentionæ–¹å¼é€å…¥æ‰©æ•£æ¨¡å‹çš„UNetä¸­ä½œä¸ºconditionï¼›
- **UNet**ï¼šæ‰©æ•£æ¨¡å‹çš„ä¸»ä½“ï¼Œç”¨æ¥å®ç°æ–‡æœ¬å¼•å¯¼ä¸‹çš„latentç”Ÿæˆã€‚

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled.webp)

## Autoencoder

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%201.png)

[latent diffusion loss](https://github.com/CompVis/latent-diffusion/tree/main/ldm/modules/losses)

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%202.png)

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%203.png)

```python
import torch
from diffusers import AutoencoderKL
import numpy as np
from PIL import Image

#åŠ è½½æ¨¡å‹: autoencoderå¯ä»¥é€šè¿‡SDæƒé‡æŒ‡å®šsubfolderæ¥å•ç‹¬åŠ è½½
autoencoder = AutoencoderKL.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="vae")
autoencoder.to("cuda", dtype=torch.float16)

# è¯»å–å›¾åƒå¹¶é¢„å¤„ç†
raw_image = Image.open("boy.png").convert("RGB").resize((256, 256))
image = np.array(raw_image).astype(np.float32) / 127.5 - 1.0
image = image[None].transpose(0, 3, 1, 2)
image = torch.from_numpy(image)

# å‹ç¼©å›¾åƒä¸ºlatentå¹¶é‡å»º
with torch.inference_mode():
Â Â Â  latent = autoencoder.encode(image.to("cuda", dtype=torch.float16)).latent_dist.sample()
Â Â Â  rec_image = autoencoder.decode(latent).sample
Â Â Â  rec_image = (rec_image / 2 + 0.5).clamp(0, 1)
Â Â Â  rec_image = rec_image.cpu().permute(0, 2, 3, 1).numpy()
Â Â Â  rec_image = (rec_image * 255).round().astype("uint8")
Â Â Â  rec_image = Image.fromarray(rec_image[0])
rec_image
```

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%204.png)

## **CLIP text encoder**

SD**é‡‡ç”¨CLIP text encoderæ¥å¯¹è¾“å…¥textæå–text embeddings**ï¼Œå…·ä½“çš„æ˜¯é‡‡ç”¨ç›®å‰OpenAIæ‰€å¼€æºçš„æœ€å¤§CLIPæ¨¡å‹ï¼š[clip-vit-large-patch14](https://link.zhihu.com/?target=https%3A//huggingface.co/openai/clip-vit-large-patch14)ï¼Œè¿™ä¸ªCLIPçš„text encoderæ˜¯ä¸€ä¸ª**transformeræ¨¡å‹ï¼ˆåªæœ‰encoderæ¨¡å—ï¼‰**ï¼šå±‚æ•°ä¸º12ï¼Œç‰¹å¾ç»´åº¦ä¸º768ï¼Œæ¨¡å‹å‚æ•°å¤§å°æ˜¯123Mã€‚å¯¹äºè¾“å…¥textï¼Œé€å…¥CLIP text encoderåå¾—åˆ°æœ€åçš„hidden statesï¼ˆå³æœ€åä¸€ä¸ªtransformer blockå¾—åˆ°çš„ç‰¹å¾ï¼‰ï¼Œå…¶ç‰¹å¾ç»´åº¦å¤§å°ä¸º77x768ï¼ˆ77æ˜¯tokençš„æ•°é‡ï¼‰ï¼Œ**è¿™ä¸ªç»†ç²’åº¦çš„text embeddingså°†ä»¥cross attentionçš„æ–¹å¼é€å…¥UNetä¸­**ã€‚åœ¨transofmersåº“ä¸­ï¼Œå¯ä»¥å¦‚ä¸‹ä½¿ç”¨CLIP text encoderï¼š

```python
from transformers import CLIPTextModel, CLIPTokenizer

text_encoder = CLIPTextModel.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="text_encoder").to("cuda")
# text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14").to("cuda")
tokenizer = CLIPTokenizer.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="tokenizer")
# tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")

# å¯¹è¾“å…¥çš„textè¿›è¡Œtokenizeï¼Œå¾—åˆ°å¯¹åº”çš„token ids
prompt = "a photograph of an astronaut riding a horse"
text_input_ids = text_tokenizer(
    prompt,
    padding="max_length",
    max_length=tokenizer.model_max_length,
    truncation=True,
    return_tensors="pt"
).input_ids

# å°†token idsé€å…¥text modelå¾—åˆ°77x768çš„ç‰¹å¾
text_embeddings = text_encoder(text_input_ids.to("cuda"))[0]
```

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œçš„tokenizeræœ€å¤§é•¿åº¦ä¸º77ï¼ˆCLIPè®­ç»ƒæ—¶æ‰€é‡‡ç”¨çš„è®¾ç½®ï¼‰ï¼Œå½“è¾“å…¥textçš„tokensæ•°é‡è¶…è¿‡77åï¼Œå°†è¿›è¡Œæˆªæ–­ï¼Œå¦‚æœä¸è¶³åˆ™è¿›è¡Œpaddingsï¼Œè¿™æ ·å°†ä¿è¯æ— è®ºè¾“å…¥ä»»ä½•é•¿åº¦çš„æ–‡æœ¬ï¼ˆç”šè‡³æ˜¯ç©ºæ–‡æœ¬ï¼‰éƒ½å¾—åˆ°77x768å¤§å°çš„ç‰¹å¾ã€‚ åœ¨è®­ç»ƒSDçš„è¿‡ç¨‹ä¸­ï¼Œ**CLIP text encoderæ¨¡å‹æ˜¯å†»ç»“çš„**ã€‚åœ¨æ—©æœŸçš„å·¥ä½œä¸­ï¼Œæ¯”å¦‚OpenAIçš„[GLIDE](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2112.10741)å’Œlatent diffusionä¸­çš„LDMå‡é‡‡ç”¨ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„tranformeræ¨¡å‹æ¥æå–textçš„ç‰¹å¾ï¼Œä½†æ˜¯æœ€æ–°çš„å·¥ä½œéƒ½æ˜¯é‡‡ç”¨é¢„è®­ç»ƒå¥½çš„text modelã€‚æ¯”å¦‚è°·æ­Œçš„Imagené‡‡ç”¨çº¯æ–‡æœ¬æ¨¡å‹T5 encoderæ¥æå‡ºæ–‡æœ¬ç‰¹å¾ï¼Œè€ŒSDåˆ™é‡‡ç”¨CLIP text encoderï¼Œé¢„è®­ç»ƒå¥½çš„æ¨¡å‹å¾€å¾€å·²ç»åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå®ƒä»¬è¦æ¯”ç›´æ¥é‡‡ç”¨ä¸€ä¸ªä»é›¶è®­ç»ƒå¥½çš„æ¨¡å‹è¦å¥½ã€‚

## UNet

SDçš„æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ä¸ª860Mçš„UNetï¼Œå…¶ä¸»è¦ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ˆè¿™é‡Œä»¥è¾“å…¥çš„latentä¸º64x64x4ç»´åº¦ä¸ºä¾‹ï¼‰ï¼Œå…¶ä¸­encoderéƒ¨åˆ†åŒ…æ‹¬3ä¸ªCrossAttnDownBlock2Dæ¨¡å—å’Œ1ä¸ªDownBlock2Dæ¨¡å—ï¼Œè€Œdecoderéƒ¨åˆ†åŒ…æ‹¬1ä¸ªUpBlock2Dæ¨¡å—å’Œ3ä¸ªCrossAttnUpBlock2Dæ¨¡å—ï¼Œä¸­é—´è¿˜æœ‰ä¸€ä¸ªUNetMidBlock2DCrossAttnæ¨¡å—ã€‚encoderå’Œdecoderä¸¤ä¸ªéƒ¨åˆ†æ˜¯å®Œå…¨å¯¹åº”çš„ï¼Œä¸­é—´å­˜åœ¨skip connectionã€‚æ³¨æ„3ä¸ªCrossAttnDownBlock2Dæ¨¡å—æœ€åå‡æœ‰ä¸€ä¸ª2xçš„downsampleæ“ä½œï¼Œè€ŒDownBlock2Dæ¨¡å—æ˜¯ä¸åŒ…å«ä¸‹é‡‡æ ·çš„ã€‚

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%205.png)

å…¶ä¸­CrossAttnDownBlock2Dæ¨¡å—çš„ä¸»è¦ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œtext conditionå°†é€šè¿‡CrossAttentionæ¨¡å—åµŒå…¥è¿›æ¥ï¼Œæ­¤æ—¶Attentionçš„queryæ˜¯UNetçš„ä¸­é—´ç‰¹å¾ï¼Œè€Œkeyå’Œvalueåˆ™æ˜¯text embeddingsã€‚ CrossAttnUpBlock2Dæ¨¡å—å’ŒCrossAttnDownBlock2Dæ¨¡å—æ˜¯ä¸€è‡´çš„ï¼Œä½†æ˜¯å°±æ˜¯æ€»å±‚æ•°ä¸º3ã€‚

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%206.png)

### UNet code

```python
class LinearAttention(nn.Module):
    def __init__(self, dim, heads=4, dim_head=32):
        super().__init__()
        self.heads = heads
        hidden_dim = dim_head * heads
        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)
        self.to_out = nn.Conv2d(hidden_dim, dim, 1)

    def forward(self, x):
        b, c, h, w = x.shape
        qkv = self.to_qkv(x)
        q, k, v = rearrange(qkv, 'b (qkv heads c) h w -> qkv b heads c (h w)', heads = self.heads, qkv=3)
        k = k.softmax(dim=-1)  
        context = torch.einsum('bhdn,bhen->bhde', k, v)
        out = torch.einsum('bhde,bhdn->bhen', context, q)
        out = rearrange(out, 'b heads c (h w) -> b (heads c) h w', heads=self.heads, h=h, w=w)
        return self.to_out(out)
```

```python
class ResnetBlock(nn.Module):
    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,
                 dropout, temb_channels=512):
        super().__init__()
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels
        self.use_conv_shortcut = conv_shortcut

        self.norm1 = Normalize(in_channels)
        self.conv1 = torch.nn.Conv2d(in_channels,
                                     out_channels,
                                     kernel_size=3,
                                     stride=1,
                                     padding=1)
        if temb_channels > 0:
            self.temb_proj = torch.nn.Linear(temb_channels,
                                             out_channels)
        self.norm2 = Normalize(out_channels)
        self.dropout = torch.nn.Dropout(dropout)
        self.conv2 = torch.nn.Conv2d(out_channels,
                                     out_channels,
                                     kernel_size=3,
                                     stride=1,
                                     padding=1)
        if self.in_channels != self.out_channels:
            if self.use_conv_shortcut:
                self.conv_shortcut = torch.nn.Conv2d(in_channels,
                                                     out_channels,
                                                     kernel_size=3,
                                                     stride=1,
                                                     padding=1)
            else:
                self.nin_shortcut = torch.nn.Conv2d(in_channels,
                                                    out_channels,
                                                    kernel_size=1,
                                                    stride=1,
                                                    padding=0)

    def forward(self, x, temb):
        h = x
        h = self.norm1(h)
        h = nonlinearity(h)
        h = self.conv1(h)

        if temb is not None:
            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]

        h = self.norm2(h)
        h = nonlinearity(h)
        h = self.dropout(h)
        h = self.conv2(h)

        if self.in_channels != self.out_channels:
            if self.use_conv_shortcut:
                x = self.conv_shortcut(x)
            else:
                x = self.nin_shortcut(x)

        return x+h

class LinAttnBlock(LinearAttention):
    """to match AttnBlock usage"""
    def __init__(self, in_channels):
        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)

class AttnBlock(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.in_channels = in_channels

        self.norm = Normalize(in_channels)
        self.q = torch.nn.Conv2d(in_channels,
                                 in_channels,
                                 kernel_size=1,
                                 stride=1,
                                 padding=0)
        self.k = torch.nn.Conv2d(in_channels,
                                 in_channels,
                                 kernel_size=1,
                                 stride=1,
                                 padding=0)
        self.v = torch.nn.Conv2d(in_channels,
                                 in_channels,
                                 kernel_size=1,
                                 stride=1,
                                 padding=0)
        self.proj_out = torch.nn.Conv2d(in_channels,
                                        in_channels,
                                        kernel_size=1,
                                        stride=1,
                                        padding=0)

    def forward(self, x):
        h_ = x
        h_ = self.norm(h_)
        q = self.q(h_)
        k = self.k(h_)
        v = self.v(h_)

        # compute attention
        b,c,h,w = q.shape
        q = q.reshape(b,c,h*w)
        q = q.permute(0,2,1)   # b,hw,c
        k = k.reshape(b,c,h*w) # b,c,hw
        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]
        w_ = w_ * (int(c)**(-0.5))
        w_ = torch.nn.functional.softmax(w_, dim=2)

        # attend to values
        v = v.reshape(b,c,h*w)
        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)
        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]
        h_ = h_.reshape(b,c,h,w)

        h_ = self.proj_out(h_)

        return x+h_

def make_attn(in_channels, attn_type="vanilla"):
    assert attn_type in ["vanilla", "linear", "none"], f'attn_type {attn_type} unknown'
    print(f"making attention of type '{attn_type}' with {in_channels} in_channels")
    if attn_type == "vanilla":
        return AttnBlock(in_channels)
    elif attn_type == "none":
        return nn.Identity(in_channels)
    else:
        return LinAttnBlock(in_channels)
```

```python
class Model(nn.Module):
    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,
                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,
                 resolution, use_timestep=True, use_linear_attn=False, attn_type="vanilla"):
        super().__init__()
        if use_linear_attn: attn_type = "linear"
        self.ch = ch
        self.temb_ch = self.ch*4
        self.num_resolutions = len(ch_mult)
        self.num_res_blocks = num_res_blocks
        self.resolution = resolution
        self.in_channels = in_channels

        self.use_timestep = use_timestep
        if self.use_timestep:
            # timestep embedding
            self.temb = nn.Module()
            self.temb.dense = nn.ModuleList([
                torch.nn.Linear(self.ch,
                                self.temb_ch),
                torch.nn.Linear(self.temb_ch,
                                self.temb_ch),
            ])

        # downsampling
        self.conv_in = torch.nn.Conv2d(in_channels,
                                       self.ch,
                                       kernel_size=3,
                                       stride=1,
                                       padding=1)

        curr_res = resolution
        in_ch_mult = (1,)+tuple(ch_mult)
        self.down = nn.ModuleList()
        for i_level in range(self.num_resolutions):
            block = nn.ModuleList()
            attn = nn.ModuleList()
            block_in = ch*in_ch_mult[i_level]
            block_out = ch*ch_mult[i_level]
            for i_block in range(self.num_res_blocks):
                block.append(ResnetBlock(in_channels=block_in,
                                         out_channels=block_out,
                                         temb_channels=self.temb_ch,
                                         dropout=dropout))
                block_in = block_out
                if curr_res in attn_resolutions:
                    attn.append(make_attn(block_in, attn_type=attn_type))
            down = nn.Module()
            down.block = block
            down.attn = attn
            if i_level != self.num_resolutions-1:
                down.downsample = Downsample(block_in, resamp_with_conv)
                curr_res = curr_res // 2
            self.down.append(down)

        # middle
        self.mid = nn.Module()
        self.mid.block_1 = ResnetBlock(in_channels=block_in,
                                       out_channels=block_in,
                                       temb_channels=self.temb_ch,
                                       dropout=dropout)
        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)
        self.mid.block_2 = ResnetBlock(in_channels=block_in,
                                       out_channels=block_in,
                                       temb_channels=self.temb_ch,
                                       dropout=dropout)

        # upsampling
        self.up = nn.ModuleList()
        for i_level in reversed(range(self.num_resolutions)):
            block = nn.ModuleList()
            attn = nn.ModuleList()
            block_out = ch*ch_mult[i_level]
            skip_in = ch*ch_mult[i_level]
            for i_block in range(self.num_res_blocks+1):
                if i_block == self.num_res_blocks:
                    skip_in = ch*in_ch_mult[i_level]
                block.append(ResnetBlock(in_channels=block_in+skip_in,
                                         out_channels=block_out,
                                         temb_channels=self.temb_ch,
                                         dropout=dropout))
                block_in = block_out
                if curr_res in attn_resolutions:
                    attn.append(make_attn(block_in, attn_type=attn_type))
            up = nn.Module()
            up.block = block
            up.attn = attn
            if i_level != 0:
                up.upsample = Upsample(block_in, resamp_with_conv)
                curr_res = curr_res * 2
            self.up.insert(0, up) # prepend to get consistent order

        # end
        self.norm_out = Normalize(block_in)
        self.conv_out = torch.nn.Conv2d(block_in,
                                        out_ch,
                                        kernel_size=3,
                                        stride=1,
                                        padding=1)

    def forward(self, x, t=None, context=None):
        #assert x.shape[2] == x.shape[3] == self.resolution
        if context is not None:
            # assume aligned context, cat along channel axis
            x = torch.cat((x, context), dim=1)
        if self.use_timestep:
            # timestep embedding
            assert t is not None
            temb = get_timestep_embedding(t, self.ch)
            temb = self.temb.dense[0](temb)
            temb = nonlinearity(temb)
            temb = self.temb.dense[1](temb)
        else:
            temb = None

        # downsampling
        hs = [self.conv_in(x)]
        for i_level in range(self.num_resolutions):
            for i_block in range(self.num_res_blocks):
                h = self.down[i_level].block[i_block](hs[-1], temb)
                if len(self.down[i_level].attn) > 0:
                    h = self.down[i_level].attn[i_block](h)
                hs.append(h)
            if i_level != self.num_resolutions-1:
                hs.append(self.down[i_level].downsample(hs[-1]))

        # middle
        h = hs[-1]
        h = self.mid.block_1(h, temb)
        h = self.mid.attn_1(h)
        h = self.mid.block_2(h, temb)

        # upsampling
        for i_level in reversed(range(self.num_resolutions)):
            for i_block in range(self.num_res_blocks+1):
                h = self.up[i_level].block[i_block](
                    torch.cat([h, hs.pop()], dim=1), temb)
                if len(self.up[i_level].attn) > 0:
                    h = self.up[i_level].attn[i_block](h)
            if i_level != 0:
                h = self.up[i_level].upsample(h)

        # end
        h = self.norm_out(h)
        h = nonlinearity(h)
        h = self.conv_out(h)
        return h

    def get_last_layer(self):
        return self.conv_out.weight
```

![Untitled](Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%207.png)

```python

import torch
from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler
from transformers import CLIPTextModel, CLIPTokenizer
import torch.nn.functional as F

# åŠ è½½autoencoder
vae = AutoencoderKL.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="vae")
# åŠ è½½text encoder
text_encoder = CLIPTextModel.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="text_encoder")
tokenizer = CLIPTokenizer.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="tokenizer")
# åˆå§‹åŒ–UNet
unet = UNet2DConditionModel(**model_config) # model_configä¸ºæ¨¡å‹å‚æ•°é…ç½®
# å®šä¹‰scheduler
noise_scheduler = DDPMScheduler(
    beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000
)

# å†»ç»“vaeå’Œtext_encoder
vae.requires_grad_(False)
text_encoder.requires_grad_(False)

opt = torch.optim.AdamW(unet.parameters(), lr=1e-4)

for step, batch in enumerate(train_dataloader):
    with torch.no_grad():
        # å°†imageè½¬åˆ°latentç©ºé—´
        latents = vae.encode(batch["image"]).latent_dist.sample()
        latents = latents * vae.config.scaling_factor # rescaling latents
        # æå–text embeddings
        text_input_ids = text_tokenizer(
            batch["text"],
            padding="max_length",
            max_length=tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt"
  ).input_ids
  text_embeddings = text_encoder(text_input_ids)[0]
    
    # éšæœºé‡‡æ ·å™ªéŸ³
    noise = torch.randn_like(latents)
    bsz = latents.shape[0]
    # éšæœºé‡‡æ ·timestep
    timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)
    timesteps = timesteps.long()

    # å°†noiseæ·»åŠ åˆ°latentä¸Šï¼Œå³æ‰©æ•£è¿‡ç¨‹
    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

    # é¢„æµ‹noiseå¹¶è®¡ç®—loss
    model_pred = unet(noisy_latents, timesteps, encoder_hidden_states=text_embeddings).sample
    loss = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")

 opt.step()
    opt.zero_grad()
```

æ³¨æ„çš„æ˜¯SDçš„noise schedulerè™½ç„¶ä¹Ÿæ˜¯é‡‡ç”¨ä¸€ä¸ª1000æ­¥é•¿çš„schedulerï¼Œä½†æ˜¯ä¸æ˜¯linearçš„ï¼Œè€Œæ˜¯scaled linearï¼Œå…·ä½“çš„è®¡ç®—å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
betas = torch.linspace(beta_start**0.5, beta_end**0.5, num_train_timesteps, dtype=torch.float32) ** 2
```

åœ¨è®­ç»ƒæ¡ä»¶æ‰©æ•£æ¨¡å‹æ—¶ï¼Œå¾€å¾€ä¼šé‡‡ç”¨**Classifier-Free Guidance**ï¼ˆè¿™é‡Œç®€ç§°ä¸ºCFGï¼‰ï¼Œæ‰€è°“çš„CFGç®€å•æ¥è¯´å°±æ˜¯åœ¨è®­ç»ƒæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åŒæ—¶ä¹Ÿè®­ç»ƒä¸€ä¸ªæ— æ¡ä»¶çš„æ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶åœ¨é‡‡æ ·é˜¶æ®µå°†æ¡ä»¶æ§åˆ¶ä¸‹é¢„æµ‹çš„å™ªéŸ³å’Œæ— æ¡ä»¶ä¸‹çš„é¢„æµ‹å™ªéŸ³ç»„åˆåœ¨ä¸€èµ·æ¥ç¡®å®šæœ€ç»ˆçš„å™ªéŸ³ï¼Œå…·ä½“çš„è®¡ç®—å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%208.png)

è¿™é‡Œçš„$\omega$ä¸º**guidance scale**ï¼Œå½“$\omega$è¶Šå¤§æ—¶ï¼Œcondition èµ·çš„ä½œç”¨è¶Šå¤§ï¼Œå³ç”Ÿæˆçš„å›¾åƒå…¶æ›´å’Œè¾“å…¥æ–‡æœ¬ä¸€è‡´ã€‚CFGçš„å…·ä½“å®ç°éå¸¸ç®€å•ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦**ä»¥ä¸€å®šçš„æ¦‚ç‡ï¼ˆæ¯”å¦‚10%ï¼‰éšæœºdropæ‰text**å³å¯ï¼Œè¿™é‡Œæˆ‘ä»¬å¯ä»¥å°†textç½®ä¸ºç©ºå­—ç¬¦ä¸²ï¼ˆå‰é¢è¯´è¿‡æ­¤æ—¶ä¾ç„¶èƒ½å¤Ÿæå–text embeddingsï¼‰ã€‚è¿™é‡Œå¹¶æ²¡æœ‰ä»‹ç»CLFèƒŒåçš„æŠ€æœ¯åŸç†ï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥é˜…è¯»CFGçš„è®ºæ–‡

[Classifier-Free Diffusion Guidance](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2207.12598) ä»¥åŠguided diffusionçš„è®ºæ–‡ [Diffusion Models Beat GANs on Image Synthesis](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2105.05233) **CFGå¯¹äºæå‡æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆæ•ˆæœæ˜¯è‡³å…³é‡è¦çš„**ã€‚

[SD åº”ç”¨](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/SD%20%E5%BA%94%E7%94%A8%202eae7a95ea4e489d9859a2f942bc2bc9.md)

## SDXL

[zhuanlan.zhihu.com](https://zhuanlan.zhihu.com/p/642496862)

[Diffusion model â€” SDXL](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Diffusion%20model%20%E2%80%94%20SDXL%20034fb5c11e2b4b82b3e5931c50a86a6a.md)

[**Stable diffusion library**](https://huggingface.co/blog/stable_diffusion)

- [**The Annotated Diffusion Model**](https://huggingface.co/blog/annotated-diffusion)
- [**Getting started with ğŸ§¨ Diffusers**](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb)

[**https://huggingface.co/docs/diffusers/index**](https://huggingface.co/docs/diffusers/index)

[Stable_Diffusion_Diagrams_V2.pdf](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Stable_Diffusion_Diagrams_V2.pdf)

# Scalable Diffusion Models with Transformers (DiT)

[](https://arxiv.org/pdf/2212.09748.pdf)

We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. 

**Diffusion Transformers, or DiTs**

## **Patchify**

The input to DiT is a spatial representation z (for 256 Ã— 256 Ã— 3 images, z has shape 32 Ã— 32 Ã— 4). The first layer of DiT is â€œpatchify,â€ which converts the spatial input into a sequence of T tokens, each of dimension d, by **linearly embedding** each patch in the input. Following patchify, we apply standard ViT **frequency-based positional embeddings (the sine-cosine version) to all input tokens**. The number of tokens T created by patchify is determined by the patch size hyperparameter p. As shown in Figure 4, halving p will quadruple T, and thus at least quadruple total transformer Gflops. Although it has a significant impact on Gflops, note that changing p has no meaningful impact on downstream parameter counts. We add p = 2, 4, 8 to the DiT design space.

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%209.png)

## DiT block design

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%2010.png)

Following patchify, the input tokens are processed by a sequence of transformer blocks. In addition to noised image inputs, diffusion models sometimes process additional conditional information such as **noise timesteps t, class labels c**, natural language, etc. We explore **four variants of transformer blocks** that process conditional inputs differently. The designs introduce small, but important, modifications to the standard ViT block design. The designs of all blocks are shown in Figure 3. 

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%2011.png)

### In-context conditioning

append the vector embeddings of **t and c** as **two additional tokens** in the input sequence, treating them no differently from the image tokens. This is similar to cls tokens in ViTs, and it allows us to use standard ViT blocks without modification. After the final block, we remove the conditioning tokens from the sequence. 

### Cross-attention block

We concatenate the embeddings of t and c into a length-two sequence, separate from the image token sequence. The transformer block is modified to include an **additional multi-head cross- attention layer following the multi-head self-attention block**, similar to the original design from Vaswani et al. [60], and also similar to the one used by LDM for conditioning on class labels.

### Adaptive layer norm (adaLN) block

Following the widespread usage of **adaptive normalization layers** [40] in GANs [2, 28] and diffusion models with U- Net backbones [9], we explore replacing standard layer norm layers in transformer blocks with adaptive layer norm (adaLN). Rather than directly learn dimension- wise **scale and shift parameters** Î³ and Î², we **regress** them from the sum of the embedding vectors of t and c. It is also the only conditioning mechanism that is restricted to **apply the same function to all tokens**.

[](https://arxiv.org/pdf/1911.07013.pdf)

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%2012.png)

### adaLN-Zero block

Zero-initializing the final convolutional layer in each block prior to any residual connections. In addition to regressing Î³ and Î², we also regress dimension- wise scaling parameters Î± that are applied immediately prior to any residual connections within the DiT block. We **initialize the MLP to output the zero-vector for all Î±; this initializes the full DiT block as the identity function.** 

### Model size

We apply a sequence of N DiT blocks, each operating at the hidden dimension size d. Following ViT, we use standard transformer configs that jointly scale N, d and attention heads

![Untitled](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Untitled%2013.png)

### Transformer decoder.

After the final DiT block, we need to decode our sequence of image tokens into an **output noise prediction and an output diagonal covariance prediction**. Both of these outputs have **shape equal** to the original spatial input. We use a standard **linear decoder** to do this; we apply the final layer norm (adaptive if using adaLN) and linearly decode each token into a pÃ—pÃ—2C tensor, where C is the number of channels in the spatial input to DiT. Finally,  we rearrange the decoded tokens into their original spatial layout to get the predicted noise and covariance. The complete DiT design space we explore is **patch size, transformer block architecture and model size**

For the rest of the paper, all models will use **adaLN-Zero DiT blocks**

[](https://github.com/facebookresearch/DiT/blob/main/models.py)

[DiT model](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/DiT%20model%20a73a071c51544374997ae8db29ed92fb.md)

[Diffusion modelï¼šDiT, transformer only](images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Diffusion%20model%EF%BC%9ADiT,%20transformer%20only%208a22b30972f343d0befe0f0a373d8387.md)
