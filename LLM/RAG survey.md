# RAG survey

RAG survey

# RAG survey

[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/pdf/2312.10997.pdf)

[https://arxiv.org/pdf/2312.10997.pdf](https://arxiv.org/pdf/2312.10997.pdf)

The progression of RAG paradigms: Naive RAG, the Advanced RAG, and the Modular RAG.

![Untitled](RAG%20survey%20900e4430df6343fb9f79cbb119b0c4b2/Untitled.png)

# Definition

![Untitled](RAG%20survey%20900e4430df6343fb9f79cbb119b0c4b2/Untitled%201.png)

ChatGPT as the most renowned and widely utilized LLM, constrained by its pretraining data, lacks knowledge of re- cent events. RAG addresses this gap by retrieving up-to-date document excerpts from external knowledge bases. In this in- stance, it procures a selection of news articles pertinent to the inquiry. TheseThese articles, alongside the initial question, are then amalgamated into an enriched prompt that enables ChatGPT to synthesize an informed response.

RAG has been enriched through various innovative approaches addressing pivotal questions such as “what to retrieve” “when to retrieve” and “how to use the retrieved information”.

# RAG FrameWork

## Naive RAG

Naive RAG follows a traditional process that includes indexing, retrieval, and gen- eration. It is also characterized as a “Retrieve-Read” frame- work 

### Indexing

1. data indexing→ standardized plain text 
2. chunking (text into smaller more manageable chuncks) → vector representation (embedding)
3. key-value pairs (chunks index vs chunks embedding vectors)

### Retrieval

1. use the indexing **encoder (embedding model)** to get input query vector.
2. compute similarity with all chunks
3. Top K chunks selection

### Generation

merge query and retrieval chunks → result

## Advanced RAG

In terms of retrieval quality, Advanced RAG implements **pre-retrieval** and **post-retrieval** strategies. To address the indexing challenges experienced by Naive RAG, Advanced RAG has re- fined its indexing approach using techniques such as **sliding window, fine-grained segmentation, and metadata**. 

### Pre-Retrieval Process Optimizing (Indexing part)

1. **Enhancing data granularity**: elevate text standardization, consistency, factual accuracy, and rich context to improve the RAG system’s performance
2. **Optimizing index structures**: adjusting the size of chunks to capture relevant context, querying across multiple index paths, and incorporating information from the graph structure to capture relevant context by leveraging relationships between nodes in a graph data index
3. **Adding metadata information**: integrating referenced metadata, such as dates and purposes, into chunks for filtering purposes, and incorporating metadata like chapters and subsections of references to improve retrieval efficiency
4. **Alignment optimization:** addresses alignment issues and disparities between documents by introducing “hypothetical questions” [Li et al., 2023d] into documents to rectify alignment issues and differences

### Retrieval

During the retrieval stage, the primary focus is on identifying the appropriate context by calculating the **similarity** between the query and chunks. The **embedding model** is central to this process. 

1. **Fine-tuning Embedding**. customizing embedding models to enhance retrieval relevance in **domain-specific contexts**, especially for professional domains dealing with evolving or rare terms
2. **Dynamic Embedding**. a sophisticated dynamic embedding model that captures contextual understanding. (BERT)

### Post-Retrieval Process

How to Merge the retrieval with the query as an input into LLMs while addressing challenges posed by context window limits?

1. **Re-Ranking :** Re-ranking the retrieved information to relocate the most relevant content to **the edges of the prompt(beginning or end)** is a key strategy.
2. **Prompt Compression**. compressing irrelevant context, highlighting pivotal paragraphs, and reducing the overall context length

## Modular RAG

It allows for either a serialized pipeline or an end-to-end training approach across multiple modules

![Untitled](RAG%20survey%20900e4430df6343fb9f79cbb119b0c4b2/Untitled%202.png)

### New Modules

1. **Search Module** (achieved using code generated by the LLM, query languages such as SQL or Cypher, and other custom tools. Multi-type data sources)
2. **Memory Module**: harnesses the memory capabilities of the LLM to guide retrieval. retrieval-enhanced generative model
3. **Fusion**. a multi-query approach. captures the explicit information users seek but also un- covers deeper, transformative knowledge. **parallel vector searches** of both original and expanded queries, intelligent **re-ranking** to optimize results, and pairing the best outcomes with new queries
4. **Routing**. Query routing decides the **subsequent action** to a user’s query, with options ranging from summarization, searching specific databases, or merging different pathways into a single response. chooses **the appropriate data store** for the query, which may include various sources like vector stores, graph databases, or relational databases, or a hierarchy of indices
5. **Predict**. It addresses the common issues of redundancy and noise in retrieved content. this module utilizes the LLM to **generate the necessary context**
6. **Task Adapter**. This module focuses on adapting RAG to a variety of downstream tasks

### **New Patterns**

2 Kinds of organizational paradigms: The first involves **adding or replacing modules**, while the second focuses on **adjusting the organizational flow** between modules.

1. **Adding or Replacing Modules.** The strategy of introducing or substituting modules involves maintaining the core structure of the Retrieval-Read process while integrating additional modules to enhance specific functionalities. (Knowledge-intensive NLP task)
2. **Adjusting the Flow between Modules.** Enhancing the interaction between language models and retrieval models. Using one module’s output to improve the functionality of another

### Optimizing the RAG Pipeline

1. Hybrid Search Exploration. intelligently integrating various techniques, including keyword-based search, semantic search, and vector search. 
2. Recursive Retrieval and Query Engine. acquiring **smaller chunks** during the initial retrieval phase to capture key semantic meanings. Subsequently, **larger chunks** containing more contextual information are provided to the LLM in later stages of the process.
3. StepBack-prompt. encourages the LLM to move away from specific instances and engage in reasoning around broader concepts and principles
4. Sub-Queries. using query engines, leveraging tree queries, utilizing vector queries, or executing simple sequential of chunks
5. Hypothetical Document Embeddings. Using the LLM, HyDE creates a hypothetical document (answer) in response to a query, embeds this document, and uses the resulting em- bedding to retrieve real documents similar to the hypothetical one.

# Retrieval

three fundamental questions: 1) How can we achieve accurate semantic representations? 2) What methods can align the semantic spaces of queries and documents? 3) How can the retriever’s output be aligned with the preferences of the Large Language Model?

## Enhancing Semantic Representations

### Chunk optimization

When managing external documents, the initial step involves breaking them down into **smaller chunks** to extract fine- grained features, which are then **embedded to represent their semantics**. Chunk size？

An appropriate **chunking strateg**y : the nature of the indexed content, the embedding model and its optimal block size, the expected length and complexity of user queries, and the specific application’s utilization of the retrieved results. 

In reality, getting precise query results involves flexibly **applying different chunking strategies**. 

1. **sliding** **window technology**, enabling layered retrieval by merg- ing globally related information across multiple retrieval processes
2. **small2big**: utilizes small text blocks during the initial search phase and subsequently provides larger related text blocks
3. **meta filtering**: leverages document metadata to enhance the filtering process
4. **graph indexing technique**, transforms entities and relationships into nodes and connections, sig- nificantly improving relevance

### Fine-tuning Embedding Models

Recent research has introduced prominent embedding models such as AngIE, Voyage, BGE,etc [Li and Li, 2023, VoyageAI, 2023, BAAI, 2023]. These models have undergone pre-training on extensive corpora.

*Task-specific fine-tuning of embedding models*

1. Domain Knowledge Fine-tuning. the dataset for embedding model fine-tuning en- compasses three principal elements: **queries, a corpus, and relevant documents**. The **dataset construction, model fine-tuning, and evaluation** phases each present distinct challenges.
2. Fine-tuning for Downstream Tasks. 

## Aligning Queries and Documents

retrievers may utilize **a single embedding model** for encoding both the query and the documents, or employ **separate models** for each. 

### Query Rewriting

Query2Doc and ITER-RETGEN leverage LLMs to create a pseudo-document by combining the origi- nal query with additional guidance [Wang et al., 2023c, Shao et al., 2023]. HyDE constructs query vectors using textual cues to generate a “hypothetical” document captur- ing essential patterns [Gao et al., 2022].

### Embedding Transformation

LlamaIndex [Liu, 2023] exemplifies this by introducing an adapter module that can be integrated following the query encoder. This adapter facilitates fine- tuning, thereby optimizing the representation of query embeddings to map them into a latent space that is more closely aligned with the intended tasks

### Fine-tuning Retrievers

utilize feedback signals from LLMs to refine retrieval models

UPRISE [Cheng et al., 2023a] also employs frozen LLMs to fine-tune the prompt retriever. Both the LLM and the re- triever take prompt-input pairs as inputs and utilize the scores provided by the LLM to supervise the retriever’s training

**supervised fine-tuning embedding models:**

• Attention Distillation. This approach employs cross- attention scores generated by the LLM during output to distill the model’s knowledge.
• EMDR2. By using the Expectation-Maximization algo- rithm, this method trains the model with retrieved docu- ments as latent variables.
• Perplexity Distillation directly trains the model using the perplexity of generated tokens as an indicator

• LOOP. This method presents a novel loss function based on the impact of document deletion on LLM prediction, offering an efficient training strategy to better adapt the model to specific tasks.

### Adapters

PRCA trains the adapter through a context extraction phase and a reward-driven phase. The retriever’s out- put is then optimized using a token-based autoregressive strategy [Yang et al., 2023b]. The token filtering approach employs cross-attention scores to efficiently filter tokens, selecting only the highest-scoring input to- kens [Berchansky et al., 2023]