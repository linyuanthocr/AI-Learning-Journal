# Project Directory Structure

  - **CNN_DL/**
    - [ResNet.md](CNN_DL/ResNet.md)
    - [deep learning on mobiles.md](CNN_DL/deep%20learning%20on%20mobiles.md)
    - [Early stopping at minimum loss.pdf](CNN_DL/Early%20stopping%20at%20minimum%20loss.pdf)
    - [Non-Maximum Suppression.pdf](CNN_DL/Non-Maximum%20Suppression.pdf)
    - [Contrastive learning.md](CNN_DL/Contrastive%20learning.md)
    - [UNet.pdf](CNN_DL/UNet.pdf)
    - [Batch normalization.pdf](CNN_DL/Batch%20normalization.pdf)
    - [Embedding & weights sharing.pdf](CNN_DL/Embedding%20&%20weights%20sharing.pdf)
    - [CLIP variances.md](CNN_DL/CLIP%20variances.md)
    - [Distilling the knowledge in a neural network.pdf](CNN_DL/Distilling%20the%20knowledge%20in%20a%20neural%20network.pdf)
    - [Finding Good Learning Rate and The One Cycle Policy..pdf](CNN_DL/Finding%20Good%20Learning%20Rate%20and%20The%20One%20Cycle%20Policy..pdf)
    - [RESNET.pdf](CNN_DL/RESNET.pdf)
    - [Bidirectional LSTM.md](CNN_DL/Bidirectional%20LSTM.md)
    - [LightGlue.md](CNN_DL/LightGlue.md)
    - [Faster R-CNN -tensorflow- _datasets.pdf](CNN_DL/Faster%20R-CNN%20-tensorflow-%20_datasets.pdf)
    - [Focal loss with multi-label implemented in keras.pdf](CNN_DL/Focal%20loss%20with%20multi-label%20implemented%20in%20keras.pdf)
    - [MAE -Masked autoencoders are scalable vision learners-.pdf](CNN_DL/MAE%20-Masked%20autoencoders%20are%20scalable%20vision%20learners-.pdf)
    - **images/**
  - **VO_VIO_VSLAM/**
    - [SVO mono code review.md](VO_VIO_VSLAM/SVO%20mono%20code%20review.md)
    - [2019.07.09_SLAM methods review.html](VO_VIO_VSLAM/2019.07.09_SLAM%20methods%20review.html)
    - [2019.08.14_MSCKF_IMU_propagation.md](VO_VIO_VSLAM/2019.08.14_MSCKF_IMU_propagation.md)
    - [2019.07.14_VINS_image_tracking.md](VO_VIO_VSLAM/2019.07.14_VINS_image_tracking.md)
    - [2019.08.07_Derivation_of_Inertial_Quaternion_Rate_Equation.md](VO_VIO_VSLAM/2019.08.07_Derivation_of_Inertial_Quaternion_Rate_Equation.md)
    - [CNN-SLAM Real-time dense monocular SLAM with learned depth prediction.md](VO_VIO_VSLAM/CNN-SLAM%20Real-time%20dense%20monocular%20SLAM%20with%20learned%20depth%20prediction.md)
    - **2019.04.08_IMU preintegration_04/**
      - [2019.04.08_IMU preintegration_04.html](VO_VIO_VSLAM/2019.04.08_IMU%20preintegration_04/2019.04.08_IMU%20preintegration_04.html)
    - **2019.04.08_IMU preintegration_03/**
      - [2019.04.08_IMU preintegration_03.html](VO_VIO_VSLAM/2019.04.08_IMU%20preintegration_03/2019.04.08_IMU%20preintegration_03.html)
    - **2019.07.19_VINS mobile code review/**
      - [vins_mobile_code_review.md](VO_VIO_VSLAM/2019.07.19_VINS%20mobile%20code%20review/vins_mobile_code_review.md)
    - **2019.04.08_IMU preintegration_02/**
      - [2019.04.08_IMU preintegration_02.html](VO_VIO_VSLAM/2019.04.08_IMU%20preintegration_02/2019.04.08_IMU%20preintegration_02.html)
    - **2019.07.11_MOLA/**
      - [MOLA_system_review.md](VO_VIO_VSLAM/2019.07.11_MOLA/MOLA_system_review.md)
    - **2019.04.05_KLT/**
      - [2019.04.05_KLT.html](VO_VIO_VSLAM/2019.04.05_KLT/2019.04.05_KLT.html)
      - **2019.04.05_KLT.resources/**
        - [15.1_Tracking__KLT.pdf](VO_VIO_VSLAM/2019.04.05_KLT/2019.04.05_KLT.resources/15.1_Tracking__KLT.pdf)
        - [Kanade-Lucas-Tomasi Tracker.pdf](VO_VIO_VSLAM/2019.04.05_KLT/2019.04.05_KLT.resources/Kanade-Lucas-Tomasi%20Tracker.pdf)
    - **2019.03.28_SVO_paper_review/**
      - [2019.03.28_SVO.html](VO_VIO_VSLAM/2019.03.28_SVO_paper_review/2019.03.28_SVO.html)
    - **2019.04.25_VINS_notes/**
      - [VINS简介.html](VO_VIO_VSLAM/2019.04.25_VINS_notes/VINS简介.html)
    - **2019.08.14_IMU_notes/**
      - [从零开始的 IMU 状态模型推导.html](VO_VIO_VSLAM/2019.08.14_IMU_notes/从零开始的%20IMU%20状态模型推导.html)
    - **2019.04.26_MSCKF flowchart/**
      - [MSCKF流程图.html](VO_VIO_VSLAM/2019.04.26_MSCKF%20flowchart/MSCKF流程图.html)
    - **2022.12.09_vins_code_review/**
      - [VINS 代码分析.html](VO_VIO_VSLAM/2022.12.09_vins_code_review/VINS%20代码分析.html)
    - **2019.04.05_LK optical flow/**
      - [2019.04.05_LK optical flow.html](VO_VIO_VSLAM/2019.04.05_LK%20optical%20flow/2019.04.05_LK%20optical%20flow.html)
    - **2019.03.25_ORBSLAM2/**
      - [2019.03.25_ORBSLAM2.html](VO_VIO_VSLAM/2019.03.25_ORBSLAM2/2019.03.25_ORBSLAM2.html)
    - **2019.03.24_PTAM/**
      - [2019.03.24_PTAM.html](VO_VIO_VSLAM/2019.03.24_PTAM/2019.03.24_PTAM.html)
    - **2019.04.08_IMU preintegration_01/**
      - [2019.04.08_IMU preintegration_01.html](VO_VIO_VSLAM/2019.04.08_IMU%20preintegration_01/2019.04.08_IMU%20preintegration_01.html)
    - **2019.06.18_opticalflow_visualization/**
      - [光流可视化.html](VO_VIO_VSLAM/2019.06.18_opticalflow_visualization/光流可视化.html)
    - **SVO mono code review/**
      - [reprojector code.md](VO_VIO_VSLAM/SVO%20mono%20code%20review%2015371bdab3cf80d68a83ccf7b3cbc302/reprojector%20code%2015471bdab3cf80eeb8dbdc63791189d7.md)
      - [Local BA.md](VO_VIO_VSLAM/SVO%20mono%20code%20review%2015371bdab3cf80d68a83ccf7b3cbc302/Local%20BA%2015971bdab3cf80b5aa6bdff442ae0235.md)
      - [SVO改进.md](VO_VIO_VSLAM/SVO%20mono%20code%20review%2015371bdab3cf80d68a83ccf7b3cbc302/SVO改进%2019071bdab3cf80c7bbb8e7641e36e09b.md)
      - [Sparse model-based image align.md](VO_VIO_VSLAM/SVO%20mono%20code%20review%2015371bdab3cf80d68a83ccf7b3cbc302/Sparse%20model-based%20image%20align%2017b71bdab3cf80759fa9f483be1157ca.md)
      - [optimizeStructure.md](VO_VIO_VSLAM/SVO%20mono%20code%20review%2015371bdab3cf80d68a83ccf7b3cbc302/optimizeStructure%2015971bdab3cf806c9d7cdadbda4ffbd0.md)
      - [pose_optimizer.md](VO_VIO_VSLAM/SVO%20mono%20code%20review%2015371bdab3cf80d68a83ccf7b3cbc302/pose_optimizer%2015971bdab3cf80a99705f12afe84bd02.md)
      - [增量方程.md](VO_VIO_VSLAM/SVO%20mono%20code%20review%2015371bdab3cf80d68a83ccf7b3cbc302/增量方程%2015471bdab3cf80a78dade984a4519df6.md)
      - [Feature alignment.md](VO_VIO_VSLAM/SVO%20mono%20code%20review%2015371bdab3cf80d68a83ccf7b3cbc302/Feature%20alignment%2017b71bdab3cf809a9791dbc5dd239a96.md)
      - [Pose & structure refinement.md](VO_VIO_VSLAM/SVO%20mono%20code%20review%2015371bdab3cf80d68a83ccf7b3cbc302/Pose%20&%20structure%20refinement%2017b71bdab3cf80cf80b5e42bd1e7b0d9.md)
      - [DepthFilter.md](VO_VIO_VSLAM/SVO%20mono%20code%20review%2015371bdab3cf80d68a83ccf7b3cbc302/DepthFilter%2017b71bdab3cf80b2ace7f0e612d2c1c4.md)
      - [Minimize reprojection error.md](VO_VIO_VSLAM/SVO%20mono%20code%20review%2015371bdab3cf80d68a83ccf7b3cbc302/Minimize%20reprojection%20error%2015471bdab3cf808fa3e9fac5261efeda.md)
      - [image align Jacobian matrix.md](VO_VIO_VSLAM/SVO%20mono%20code%20review%2015371bdab3cf80d68a83ccf7b3cbc302/image%20align%20Jacobian%20matrix%2015471bdab3cf80bdaf9edaed4b753a85.md)
    - **2019.03.22_SVO code/**
      - [2019.03.22_SVO code.html](VO_VIO_VSLAM/2019.03.22_SVO%20code/2019.03.22_SVO%20code.html)
  - **Tools_Usage/**
    - [Hugging face library usage.pdf](Tools_Usage/Hugging%20face%20library%20usage.pdf)
    - [Mac 中 PyCharm 配置 Anaconda环境.pdf](Tools_Usage/Mac%20中%20PyCharm%20配置%20Anaconda环境.pdf)
  - **Diffusion_models_GAN/**
    - [style transfer.md](Diffusion_models_GAN/style%20transfer.md)
    - [DALLE2: Hierarchical Text-Conditional Image Genera.pdf](Diffusion_models_GAN/DALLE2:%20Hierarchical%20Text-Conditional%20Image%20Genera.pdf)
    - [InstructPix2Pix Learning to Follow Image Editing I.md](Diffusion_models_GAN/InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%20873196964e104d4aa1fae9a777fd37f4.md)
    - [Diffusion models for video generation.md](Diffusion_models_GAN/Diffusion%20models%20for%20video%20generation%204a81adda3eb6405cbe8db8685390307b.md)
    - [Drag the GAN.pdf](Diffusion_models_GAN/Drag%20the%20GAN.pdf)
    - [Diffusion models.pdf](Diffusion_models_GAN/Diffusion%20models.pdf)
    - [High-Resolution Image Synthesis with Latent Diffus.pdf](Diffusion_models_GAN/High-Resolution%20Image%20Synthesis%20with%20Latent%20Diffus.pdf)
    - [Diffusion models for video generation.md](Diffusion_models_GAN/Diffusion%20models%20for%20video%20generation%20f8d348a799d24af0aaffeffbf24a86c8.md)
    - [StableDiffussion.md](Diffusion_models_GAN/StableDiffussion.md)
    - [VQVAE.pdf](Diffusion_models_GAN/VQVAE.pdf)
    - [What are Diffusion Models.md](Diffusion_models_GAN/What%20are%20Diffusion%20Models.md)
    - [Stable diffusion 3.md](Diffusion_models_GAN/Stable%20diffusion%203%20c73bab85d94944edbb991761fe6b6d06.md)
    - [Understanding Variational Autoencoders (VAEs).pdf](Diffusion_models_GAN/Understanding%20Variational%20Autoencoders%20(VAEs).pdf)
    - [VQVAE_notes.md](Diffusion_models_GAN/VQVAE_notes.md)
    - [Popular Diffusion models.md](Diffusion_models_GAN/Popular%20Diffusion%20models.md)
    - [DDPM.pdf](Diffusion_models_GAN/DDPM.pdf)
    - [UNet_code.md](Diffusion_models_GAN/UNet_code.md)
    - **images/**
      - **Popular Diffusion models/**
        - [Stable_Diffusion_Diagrams_V2.pdf](Diffusion_models_GAN/images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Stable_Diffusion_Diagrams_V2.pdf)
        - [SD 应用.md](Diffusion_models_GAN/images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/SD%20应用%202eae7a95ea4e489d9859a2f942bc2bc9.md)
        - [Diffusion model：DiT, transformer only.md](Diffusion_models_GAN/images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Diffusion%20model：DiT,%20transformer%20only%208a22b30972f343d0befe0f0a373d8387.md)
        - [Diffusion model   SDXL.md](Diffusion_models_GAN/images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/Diffusion%20model%20—%20SDXL%20034fb5c11e2b4b82b3e5931c50a86a6a.md)
        - [DiT model.md](Diffusion_models_GAN/images/Popular%20Diffusion%20models%20eb9e858a18874bee9cafc7e276e9e701/DiT%20model%20a73a071c51544374997ae8db29ed92fb.md)
      - **What are Diffusion Models/**
        - [Stable diffusion models.md](Diffusion_models_GAN/images/What%20are%20Diffusion%20Models%202cbceaf971814033a4183050ff6ceb35/Stable%20diffusion%20models%20719dcc3061ae4466a2ae5fea4f0cbea8.md)
      - **style transfer/**
        - [Stylebrush.md](Diffusion_models_GAN/images/style%20transfer%2014771bdab3cf80e6b5ecd51c18808e31/Stylebrush%20bd9a4a57c7574ae2b54776ba6209fcc6.md)
  - **Transformers&beyond/**
    - [Transformer- attention is all you need.pdf](Transformers&beyond/Transformer-%20attention%20is%20all%20you%20need.pdf)
    - [MOCO_ Momentum Contrast for Unsupervised Visual Re.md](Transformers&beyond/MOCO_%20Momentum%20Contrast%20for%20Unsupervised%20Visual%20Re.md)
    - [Knowledge_Dis.md](Transformers&beyond/Knowledge_Dis.md)
    - [ViT.md](Transformers&beyond/ViT.md)
    - [Flash Attention.md](Transformers&beyond/Flash%20Attention.md)
    - [S4 model Long Sequences with Structured State Spac.md](Transformers&beyond/S4%20model%20Long%20Sequences%20with%20Structured%20State%20Spac.md)
    - [LoFTR：Detector-Free Local Feature Matching.md](Transformers&beyond/LoFTR：Detector-Free%20Local%20Feature%20Matching.md)
    - [Transformer Family 2 0.md](Transformers&beyond/Transformer%20Family%202%200.md)
    - [Distillation.md](Transformers&beyond/Distillation.md)
    - [Transformer Code.pdf](Transformers&beyond/Transformer%20Code.pdf)
    - [Segment anything.pdf](Transformers&beyond/Segment%20anything.pdf)
    - [IMAGEBIND.md](Transformers&beyond/IMAGEBIND.md)
    - [Distilling the knowledge in a neural network.pdf](Transformers&beyond/Distilling%20the%20knowledge%20in%20a%20neural%20network.pdf)
    - [Multimodal Foundation Models.md](Transformers&beyond/Multimodal%20Foundation%20Models.md)
    - [Mamba.md](Transformers&beyond/Mamba.md)
    - [ImageBind.pdf](Transformers&beyond/ImageBind.pdf)
    - **images/**
      - **Multimodal Foundation Models/**
        - [Visual generation.md](Transformers&beyond/images/Multimodal%20Foundation%20Models%20eb1d711009044c5b8f3e782c1dad1d9e/Visual%20generation%20bd9cf610d0e946e58ff02c22d93bcb93.md)
        - [visual and vision-language understanding.md](Transformers&beyond/images/Multimodal%20Foundation%20Models%20eb1d711009044c5b8f3e782c1dad1d9e/visual%20and%20vision-language%20understanding%20d43dfbc1990d4276a12b9936dd4ae2db.md)
        - **visual and vision-language understanding/**
          - [NCE & InfoNCE.md](Transformers&beyond/images/Multimodal%20Foundation%20Models%20eb1d711009044c5b8f3e782c1dad1d9e/visual%20and%20vision-language%20understanding%20d43dfbc1990d4276a12b9936dd4ae2db/NCE%20&%20InfoNCE%20a38eeab6f53e484996aa6eb06260c318.md)
          - [Image-Only Self-Supervised Learning.md](Transformers&beyond/images/Multimodal%20Foundation%20Models%20eb1d711009044c5b8f3e782c1dad1d9e/visual%20and%20vision-language%20understanding%20d43dfbc1990d4276a12b9936dd4ae2db/Image-Only%20Self-Supervised%20Learning%206fb00261a5204842b3ce0e4790e0dc5f.md)
          - [Multimodal Fusion, Region-Level and Pixel-Level Pr.md](Transformers&beyond/images/Multimodal%20Foundation%20Models%20eb1d711009044c5b8f3e782c1dad1d9e/visual%20and%20vision-language%20understanding%20d43dfbc1990d4276a12b9936dd4ae2db/Multimodal%20Fusion,%20Region-Level%20and%20Pixel-Level%20Pr%201c4edfe7d5f54685b0090002fbbcd7f0.md)
          - [CLIP & CLIP Variance.md](Transformers&beyond/images/Multimodal%20Foundation%20Models%20eb1d711009044c5b8f3e782c1dad1d9e/visual%20and%20vision-language%20understanding%20d43dfbc1990d4276a12b9936dd4ae2db/CLIP%20&%20CLIP%20Variance%20bf0bbb65afc2404082cbd0a0e204aff8.md)
          - [MOCO.md](Transformers&beyond/images/Multimodal%20Foundation%20Models%20eb1d711009044c5b8f3e782c1dad1d9e/visual%20and%20vision-language%20understanding%20d43dfbc1990d4276a12b9936dd4ae2db/MOCO%2003030c7d88624a16b9fe2ad3b89a583a.md)
          - [Synergy Among Different Learning Approaches.md](Transformers&beyond/images/Multimodal%20Foundation%20Models%20eb1d711009044c5b8f3e782c1dad1d9e/visual%20and%20vision-language%20understanding%20d43dfbc1990d4276a12b9936dd4ae2db/Synergy%20Among%20Different%20Learning%20Approaches%204c4c740680e84058a76485923aadcda3.md)
          - **CLIP & CLIP Variance/**
            - [MAE.md](Transformers&beyond/images/Multimodal%20Foundation%20Models%20eb1d711009044c5b8f3e782c1dad1d9e/visual%20and%20vision-language%20understanding%20d43dfbc1990d4276a12b9936dd4ae2db/CLIP%20&%20CLIP%20Variance%20bf0bbb65afc2404082cbd0a0e204aff8/MAE%20f78945188a5a412ab297ed30a91de285.md)
            - **MAE/**
              - [MAE training code.md](Transformers&beyond/images/Multimodal%20Foundation%20Models%20eb1d711009044c5b8f3e782c1dad1d9e/visual%20and%20vision-language%20understanding%20d43dfbc1990d4276a12b9936dd4ae2db/CLIP%20&%20CLIP%20Variance%20bf0bbb65afc2404082cbd0a0e204aff8/MAE%20f78945188a5a412ab297ed30a91de285/MAE%20training%20code%20f1781e266c194a27a0ca5e831a7a9d58.md)
        - **Visual generation/**
          - [Spatial Controllable Generation.md](Transformers&beyond/images/Multimodal%20Foundation%20Models%20eb1d711009044c5b8f3e782c1dad1d9e/Visual%20generation%20bd9cf610d0e946e58ff02c22d93bcb93/Spatial%20Controllable%20Generation%20357b6ceca40c4a15b45416832f2e7dc7.md)
          - **Spatial Controllable Generation/**
            - [Adding Conditional Control to Text-to-Image Diffus.md](Transformers&beyond/images/Multimodal%20Foundation%20Models%20eb1d711009044c5b8f3e782c1dad1d9e/Visual%20generation%20bd9cf610d0e946e58ff02c22d93bcb93/Spatial%20Controllable%20Generation%20357b6ceca40c4a15b45416832f2e7dc7/Adding%20Conditional%20Control%20to%20Text-to-Image%20Diffus%20a2b155fd5a084973bf192419ae714678.md)
      - **Transformer Family 2 0/**
        - [Swin Transformer.md](Transformers&beyond/images/Transformer%20Family%202%200%20914e4aab3f78490bbf8769d797f92961/Swin%20Transformer%203f145133347d4864bad610512b237b2d.md)
  - **3D Avatar/**
    - [monocular depth estimation.md](3D%20Avatar/monocular%20depth%20estimation.md)
    - [Gaussian Head Avatar.md](3D%20Avatar/Gaussian%20Head%20Avatar.md)
    - [FLAME：Faces Learned with an Articulated Model.md](3D%20Avatar/FLAME：Faces%20Learned%20with%20an%20Articulated%20Model.md)
    - [HeadGAP.md](3D%20Avatar/HeadGAP.md)
    - [Humanoid robot.md](3D%20Avatar/Humanoid%20robot.md)
    - [3D human generation project.md](3D%20Avatar/3D%20human%20generation%20project.md)
    - [NeRF Representing Scenes as Neural Radiance Fields.md](3D%20Avatar/NeRF%20Representing%20Scenes%20as%20Neural%20Radiance%20Fields.md)
    - **images/**
      - **3D human generation project/**
        - [IDEAS.md](3D%20Avatar/images/3D%20human%20generation%20project%201a571bdab3cf80168359e46bb6a8d0b4/IDEAS%201a771bdab3cf80278398e25bf7099fd3.md)

  - **LLM/**
    - [GPT2 code.pdf](LLM/GPT2%20code.pdf)
    - [PaLM- Scaling Language Modeling with Pathways.pdf](LLM/PaLM-%20Scaling%20Language%20Modeling%20with%20Pathways.pdf)
    - [LORA.md](LLM/LORA.md)
    - [Generalized Language Models.md](LLM/Generalized%20Language%20Models.md)
    - [PALM.md](LLM/PALM.md)
    - [Self_instruct.md](LLM/Self_instruct.md)
    - [RAG survey.md](LLM/RAG%20survey.md)
    - [GPT序列.pdf](LLM/GPT序列.pdf)
    - [Instruct GPT.md](LLM/Instruct%20GPT.md)
    - [Prompt Engineering | Lil'Log.pdf](LLM/Prompt%20Engineering%20|%20Lil'Log.pdf)
    - [LLaMA- Open and Efficient Foundation Language Models.pdf](LLM/LLaMA-%20Open%20and%20Efficient%20Foundation%20Language%20Models.pdf)
      
  - **Generative AI/**
    - [Training lora.md](Generative%20AI/Training%20lora.md)
    - [12 days of no-cost generative AI training Google.md](Generative%20AI/12%20days%20of%20no-cost%20generative%20AI%20training%20Google.md)
    - [Lora training.md](Generative%20AI/Lora%20training.md)
    - [Generative AI.md](Generative%20AI/Generative%20AI.md)
    - [Generative AI for Beginners.md](Generative%20AI/Generative%20AI%20for%20Beginners.md)
