# Swin Transformer

## Architecture

![Untitled](Swin%20Transformer%203f145133347d4864bad610512b237b2d/Untitled.png)

## Method

![Untitled](Swin%20Transformer%203f145133347d4864bad610512b237b2d/Untitled%201.png)

**Calculation**

![Untitled](Swin%20Transformer%203f145133347d4864bad610512b237b2d/Untitled%202.png)

## Shifted window based Transformer

![Untitled](Swin%20Transformer%203f145133347d4864bad610512b237b2d/Untitled%203.png)

![Untitled](Swin%20Transformer%203f145133347d4864bad610512b237b2d/Untitled%204.png)

As illustrated in Figure 2, the first module uses a regular window partitioning strategy which starts from the top-left 2 × 2 windows of size 4 × 4 (M = 4). Then, the next mod- pixel, and the 8 × 8 feature map is evenly partitioned into ule adopts a windowing configuration that is shifted from that of the preceding layer, by displacing the windows by ([M/2], [M/2]) With the shifted window partitioning approach, consecutive Swin Transformer blocks are computed as

![Untitled](Swin%20Transformer%203f145133347d4864bad610512b237b2d/Untitled%205.png)

The shifted window partitioning approach introduces connections between neighboring non-overlapping win- dows in the previous layer and is found to be effective in im- age classification, object detection, and semantic segmentation, as shown in Table 4

### Efficient batch computation for shifted configuration

![Untitled](Swin%20Transformer%203f145133347d4864bad610512b237b2d/Untitled%206.png)

### left bottom part mask

![Untitled](Swin%20Transformer%203f145133347d4864bad610512b237b2d/Untitled%207.png)

![Untitled](Swin%20Transformer%203f145133347d4864bad610512b237b2d/Untitled%208.png)

### top right mask

![Untitled](Swin%20Transformer%203f145133347d4864bad610512b237b2d/Untitled%209.png)

[https://www.bilibili.com/video/BV13L4y1475U/?spm_id_from=333.337.search-card.all.click](https://www.bilibili.com/video/BV13L4y1475U/?spm_id_from=333.337.search-card.all.click)