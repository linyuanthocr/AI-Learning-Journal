# MAE

# Abstract

This paper shows that **masked autoencoders (MAE)** are scalable self-supervised learners for computer vision. Our MAE approach is simple: we **mask random patches** of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an **asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset ofpatches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens.** Second, we find that masking a high proportion of the input image, e.g., **75%**, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and ef- fectively: **we accelerate training (by 3× or more) and improve accuracy**. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre- training and shows promising scaling behavior.

![Untitled](MAE%20f78945188a5a412ab297ed30a91de285/Untitled.png)

**MAE encoder.** 

Our encoder is a ViT [16] but applied **only on visible, unmasked patches**. Just as in **a standard ViT**, our encoder embeds patches by a linear projection with added positional embeddings, and then processes the resulting set via a series of Transformer blocks. However, our encoder only operates on a small subset (e.g., 25%) of the full set. **Masked patches are removed; no mask tokens are used.** This allows us to train very large encoders with only a frac- tion of compute and memory. The full set is handled by a lightweight decoder, described next.

**MAE decoder**: 

The input to the MAE decoder is the full set of tokens consisting of **(i) encoded visible patches, and (ii) mask tokens.** See Figure 1. **Each mask token [14] is a shared**, **learned vector** that indicates the presence of a missing patch to be predicted. We **add positional embeddings to all tokens** in this full set; without this, mask tokens would have no information about their location in the image. The decoder has **another series of Transformer blocks**

The **MAE decoder is only used during pre-training** to perform the image reconstruction task (**only the encoder is used to produce image representations for recognition**). Therefore, the decoder architecture can be flexibly designed in a manner that is independent of the encoder design. We experiment with *very small decoders, narrower and shallower than the encoder*. For example, our default decoder *has <10% computation per token vs. the encoder*. With this asymmetrical design, the full set of tokens are only processed by the lightweight decoder, which significantly reduces pre-training time.

**Reconstruction target**

Our MAE reconstructs the input by predicting the pixel values for each masked patch. Each element in the decoder’s output is a vector of pixel values representing a patch. The last layer of the decoder is a linear projection whose number of output channels equals the number of pixel values in a patch. The decoder’s output is reshaped to form a reconstructed image. Our loss function computes the **mean squared error (MSE) between the reconstructed and original images in the pixel space**. We **compute the loss only on masked patches**, similar to BERT

[MAE training code](MAE%20f78945188a5a412ab297ed30a91de285/MAE%20training%20code%20f1781e266c194a27a0ca5e831a7a9d58.md)

[视觉无监督学习新范式：MAE](https://zhuanlan.zhihu.com/p/432984431)